# 31-Day-of-Data-Analytic-Project

----------------------------------------------------------------------------------------------------------------------------------------

Day  | Project    | Tool Used
-----| ----------- | --------- 
1    | [BlinkIt Sales Analysis](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%201%20Blinkit%20SQL%20Project)   | SQL
2    | [IMDB Movie Analysis](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%202%20IMDB%20Movie%20Analysis%20Python%20Project)    | Python
3    | [Analysis of Highest Paying IT Jobs In India](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%203%20Analysis%20of%20Highest%20IT%20Paying%20Jobs%20in%20India)    | Python
4    | [Understanding of Excel](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%204%20Understanding%20of%20Excel)    | Excel
5    | [World Mining Commodities](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%205%20World%20Minning%20Commodities)  | SQL
6    | [Vrinda Store Sales Analysis](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%206%20Store%20Sales%20Analysis%20using%20Excel)  | Excel & SQL
7    | [Pizza Sales Analysis](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%207%20Pizza%20Sales%20Analysis)  | PowerBI & SQL
8    | [Customer Churn Analysis](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%208%20Teco%20Customer%20Churn%20Anlaysis%20Python)  | Python
9    | [BlinkIt Grocery Sales Analysis](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%209%20BlinkIt%20Grocery%20Sales%20Excel)    | Excel
10   | [Electric Vehicle Data Analysis](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%2010%20Electric%20Vehicle%20Data%20Analysis)  | [Tableau](https://public.tableau.com/app/profile/aayush.chhettri/viz/EVDataAnalysis_17357209335390/EVDataAnalysis)
11   | [Automobile Pricing Analysis](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%2011%20Car%20Pricing%20Analysis%20Python)    | Python(Data Wrangling, Model Development)
12   | [Diwali Sales Analysis](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%2012%20Diwali%20Sales%20Analysis)    | Excel, SQL, Python, Power BI
13   | [Laptop Pricing Analysis](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%2013%20Laptop%20Pricing%20Dataset)   | Python(EDA, Model Development)
14   | [Model Evaluation and Refinement](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%2014%20Model%20Evaluation%20and%20Refinement%20Automobile%20Pricing)  | Python(Model Evaluation, Over-fitting, Under-fitting and Model Selection, Ridge Regression,Grid Search)
15   | [Heart Failure Prediction](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%2015%20Hearth%20Failure%20Prediction)  | Machine Learning (Random Forest, Desicion Tree, SVM, K-nearest Neighbor, Naive Bayes)
16   | [Target Sales Analysis](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%2016%20Target%20Sales%20Analysis)      | SQL & Python
17   | [Coffee Sales Report](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%2017%20Coffee%20Sales%20Analysis)        | Power BI, Excel & SQL
18   | [Bank Loan Report](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%2018%20Bank%20Loan%20Report%20Excel)        | Excel & Power BI
19   | [CarDekho Price Prediction](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%2019%20CarDekho%20Price%20Prediction)   | Python
20   | [HR Analytic Dashboard](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%2020%20HR%20Dashboard%20Tableau)            | [Tableau](https://public.tableau.com/app/profile/aayush.chhettri/viz/HRAnalyticDashboard_17386802426140/HRDashboard)
21   | [Spotify Music Streaming Dataset Analysis](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%2021%20Spotify%20Data%20Analysis%20using%20SQL)   | SQL
22   | [Black Friday Sales Prediction](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%2022%20Black%20Friday%20Sales%20Prediction)   | Machine Learning (Decision Tree, Random Forest, Extra Trees, XGBRegressor, Ridge, Linear Regression)
23   | [IPL Data Analysis 2008-2022](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%2023%20IPL%20Game%20Analysis)  | Power BI
24   | [WebScraping](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/tree/main/Day%2024%20Scraping%20Data%20from%20websites)    | Python

------------------------------------------------------------------------------------------------------------------------------------------------
## Day 1: BlinkIt E-commerce Sales Analysis
Shop online for groceries and get your order delivered at your doorstep in minutes. Enjoy instant delivery with blinkit.
Blinkit primarily delivers groceries, fresh fruits, vegetables, meat, stationery, bakery items, personal care, baby care and pet care products, snacks, flowers, etc.

![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/4aa8eac983570b6cbdc441123b93f6359fa656a6/Day%201%20Blinkit%20SQL%20Project/Image/Blink%20it%20profile.jpeg)

### Features
The data is available in 8 csv files:

- customers.csv
- orderdetails.csv
- orders.csv

### Objectives
This project focused on addressing key business challenges for Blinkit, including revenue optimization, customer retention, and operational efficiency. The main goals were:

1. Revenue Insights: Pinpoint top-performing products and cities generating the highest revenue.
2. Customer Analysis: Understand customer behavior by ranking customers based on spending and identifying inactive customers.
3. Order Trends: Uncover patterns in order statuses (delivered, canceled) and customer behavior, like placing multiple orders in a day.
4. Operational Insights: Optimize service delivery and improve operational efficiency by analyzing location-specific purchase data.

### Dataset Schema
![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/4aa8eac983570b6cbdc441123b93f6359fa656a6/Day%201%20Blinkit%20SQL%20Project/Image/Data%20Model.png)

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## Day 2: IMDB Movie Analysis
IMDb (an initialism for Internet Movie Database) is an online database of information related to films, television series, podcasts, home videos, video games, and streaming content online – including cast, production crew and personal biographies, plot summaries, trivia, ratings, and fan and critical reviews.

![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/e41ea29d14977fd99cd1b3a7ff34d807e46c7766/Day%202%20IMDB%20Movie%20Analysis%20Python%20Project/Images/IMDB.png)

### Dataset
The dataset used in this analysis can be found in this repository files : imdb_movies.csv.

### Objective
This project analyzes the IMDB Movie Dataset to uncover trends, popular genres, and factors influencing movie success. Using Python and libraries like Pandas, Numpy, Matplotlib and Seaborn, the analysis delivers actionable insights through comprehensive data exploration and visualization.

### Task Covered
The task description of this analysis can be found in this repository files : Task Covered.

1. Project Setup and Data Loading
2. Data Overview and Cleaning
3. Univariate and Bivariate Analysis
4. Genre-Specific Analysis
5. Yearly and Decadal Trends
6. Insights and Summary
7. Documentation

---------------------------------------------------------------------------------------------------------------------------------------

## Day 3: Analyis of Highest Paying IT Jobs in India
This project anlayzed the highest paying IT jobs in India, focusing on job positions, location, salaries, education, and experience levels. It provides insights into salary trends, correltations and distributions, helping to understand the IT sector's job market and the factors influencing compensation.

### Dataset
The dataset used in this analysis can be found in this repository file: position_salary.csv

### Objective
This project provides key insights into the IT job market in India, analyzing factors like job positions, salary trends, and the impact of education and experience on compensation. It helps professionals understand what roles offer the highest pay and the essential skills required for those positions. The analysis will focus on job positions, locations, salaries, educational qualifications, and experience levels.

### Task Covered
1. Project Setup and Data Loading
2. Exploratory Data Analysis
3. Data Cleaning
4. Solved Basic Level Quesitons and Interpretation
6. Solved Intermediate Level Question and Interpretation


 ------------------------------------------------------------------------------------------------------------------------------------------------

 ## Day 4: Understanding of Excel
Microsoft Excel is a spreadsheet editor developed by Microsoft.
 ![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/19d1076eb24e6f532326061cbc2b9869194a5744/Day%204%20Understanding%20of%20Excel/Excel.jpg)

 This section content the understanding of Excel and its capability. (PS: I used my own created data for understanding and using of different excel formulas)
   - Excel Formula: SUM, SUMIF, SUMIFs, COUNT, COUNTIF, COUNTIFs, MAX, MIN, CONCATENATE, RIGHT, LEFT, LEN, NETWORKDAYS
   - Condition Formula: IF, IFs,Condition Formatting
   - Date Formattion; Date-to-Text, Text-to-Date
   - XLOOKUP & VLOOKUP
   - Pivot Table

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

## Day  5: World Mining Commodities
This project is a comprehensive SQL-based analysis of the mining industry ideal for beginners and intermediate learners.
This project is centered around analyzing them mining industry using real-world dataset from the World Mining Commodities dataset repository. It involoes exploring the mining companies, their operaitonal data, and country-level mining statistics to generate insights and develop SQL proficiency.
![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/57e9fc3acbb2c77e003c520f2f88221a0ca40171/Day%205%20World%20Minning%20Commodities/Cover%20Image.jpg)

### Objective
To analyze global mining trends, understand commodity production levels, and evaluate company performance using SQL.

### Dataset
The dataset used in this analysis can be found in this repository file:
 i. 116_world_mining_companies_clean.csv
 ii. world_mining_commodities_clean.csv


------------------------------------------------------------------------------------------------------------------------------------------------------------

## Day 6: Vrinda Store Sales Anlaysis (Excel & SQL)

I analyzed 30,000+ records from Vrinda Store, an online clothing retailer to develop a data-driven annual sales report. This project present insights from the analysis of Vrinda Store's 2022 data, integrating sales performance, customer demographics, and operational metrics. 
![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/30bdab627a1a0475709bfe5b86172370c5baf0a5/Day%206%20Store%20Sales%20Analysis%20using%20Excel/Image/Excel%20Dashboard.png)

### Dataset
The dataset used in this analysis can be found in this repository file: Vrinda Store Data Analysis Excel.xlsx

### Objective
The objective is to create a comprehensive annual sales report to guide strategic decisions and enhance sales in 2023.

### Steps and Method:
🔹Data Cleaning and Preprocessing: Removed duplicates, corrected errors, structured sales data for analysis
🔹Data Analysis: Identified sales trends, top-performing products, and customer preference
🔹Data Visualization & Reports: Created interactive dashboard using hashtag#Excel (pivot tables, charts, & advanced formulas)
🔹SQL (CTE, Group By, Aggregate Functions, Rank Functions)
🔹Insights & Documentation: Provided actionable insights for data-driven decision-making


### Key Insight
1. Adults are the largest consumer group, followed by young and the elderly ones.
  ![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/30bdab627a1a0475709bfe5b86172370c5baf0a5/Day%206%20Store%20Sales%20Analysis%20using%20Excel/Image/Age%20Group%20%26%20Sales.png)
3. Sales decline month over month, singaling a need for proactive stategies.
4. Amazon tops the charts as the best-selling partner.
5. Maharashtra emerges as the top-performing state with sales of ₹2,990,221.
6. March recorded the highest revenue of ₹1,928,066 with 2,819 orders, indicating seasonal trends.
7. Order Status: 28,641 orders delivered successfully, while only 844 were cancelled and 517 were refunded, showcasing high delivery efficiency
![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/30bdab627a1a0475709bfe5b86172370c5baf0a5/Day%206%20Store%20Sales%20Analysis%20using%20Excel/Image/Top%205%20States.png)

----------------------------------------------------------------------------------------------------------------------------------------------------------------------

## Day 7: Pizza Sales Analysis (PowerBI & SQL)
This project involves leveraging SQL queries & building PowerBI Dashboard to analyze 𝐩𝐢𝐳𝐳𝐚 𝐬𝐚𝐥𝐞𝐬 data, aiming to uncover valuable insights that drive strategic decision-making and optimize business operations.

![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/b1376f705d811a9aaee27cc79f11fe8fa44fa7e1/Day%207%20Pizza%20Sales%20Analysis/Image/Pizza%20Sales%20Home%20Dashboard.png)

### Dataset
The dataset used in this analysis can be found in this repository file: [pizza_sales.csv](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/b1376f705d811a9aaee27cc79f11fe8fa44fa7e1/Day%207%20Pizza%20Sales%20Analysis/pizza_sales.csv)


𝐎𝐛𝐣𝐞𝐜𝐭𝐢𝐯𝐞:-  
The objective of this project is to analyze 𝐩𝐢𝐳𝐳𝐚 🍕 𝐬𝐚𝐥𝐞𝐬 data to identify trends and provide actionable insights that can help to increase sales and aim to uncover key metrics and patterns within the sales data by leveraging SQL queries & by building PowerBI Dashboard

![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/b1376f705d811a9aaee27cc79f11fe8fa44fa7e1/Day%207%20Pizza%20Sales%20Analysis/Image/Pizza%20Sales%20Performance%20Report.png)


### Key Highlights: 

 - Total revenue: $817860.05
 - Expensive pizza: The Greek pizza 
 - Top Category: Classic
 - Busiest Hour: 12:00
 - Average Order Value: $17
 - Highest Revenue Contribution: Classic Pizza

### Insights: 

- Large pizzas are the most commonly ordered size, accounting for 𝟏𝟖𝟓𝟐𝟔 orders.
- The Classic Deluxe Pizza is the most ordered type, totaling 𝟐𝟒𝟓𝟑 orders.
- Classic pizza dominates total revenue with a contribution of 𝟐𝟔.𝟗𝟏%.
- The 𝐓𝐡𝐚𝐢 𝐂𝐡𝐢𝐜𝐤𝐞𝐧 𝐏𝐢𝐳𝐳𝐚 emerges as a top revenue generator.
- 𝐀𝐯𝐞𝐫𝐚𝐠𝐞 𝐩𝐢𝐳𝐳𝐚 sales stand at 𝟏𝟑𝟖.
- The Thai Chicken Pizza ($43434.25), Barbecue Chicken Pizza ($42768) and California Chicken Pizza ($41409.5) generate the highest revenue.


-----------------------------------------------------------------------------------------------------------------------------------------------------------

## Day 8: Techno Customer Churn Analysis(Python)

#### Objective: 
The analysis explores customer churn patterns, focusing on various factors such as payment methods, contract types, tenure, and demographic attributes. The goal is to identify which factors are most strongly associated with higher churn rates to guide customer retention strategies.

#### Dataset
The dataset used in this analysis can be found in this repository file: [Customer Churn.csv](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/main/Day%208%20Teco%20Customer%20Churn%20Anlaysis%20Python/Customer%20Churn.csv)


 ### Key Insights & Findings: 
 
● Contract Type and Churn: 
    - Customers on month-to-month contracts exhibit the highest churn rate, with 42% of such customers likely to churn. 
    - In contrast, customers on one-year and two-year contracts have churn rates of 11% and 3%, respectively. 
    - Implication: Longer contract periods serve as a strong retention tool, as customers with extended commitments are far less likely to leave. 
![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/d953507a20e98a4c928504c75db4770777904374/Day%208%20Teco%20Customer%20Churn%20Anlaysis%20Python/Images/Customers%20by%20Contract.jpg)

● Payment Methods and Churn: 
   - Customers paying via electronic checks show the highest churn rate at 45%, while those using credit cards, bank transfers, or mailed checks have significantly lower churn rates, averaging around 15-18%. 
   - Implication: The convenience, security, and trust issues related to electronic payments might be contributing factors. Encouraging customers to switch to more stable payment methods could reduce churn.
![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/d953507a20e98a4c928504c75db4770777904374/Day%208%20Teco%20Customer%20Churn%20Anlaysis%20Python/Images/Customers%20by%20PaymentMethod.png)

 ● Churn by Tenure: 
   - Customers with less than one year of tenure are the most likely to churn, with a 50% churn rate. Those with 1-3 years of tenure show a decreasing churn trend at 35%, while customers who have been with the company for more than three years have a churn rate of just 15%. 
   - Implication: Engaging customers early in their journey, especially within the first year, is critical for retention.
     
● Churny Internet Service Type:
   - Customers using Fiber Optic services show a higher churn rate of 30%, compared to DSL customers with a churn rate of 20%. 
   - Implication: This could be due to increased competition or dissatisfaction with service quality. Understanding customer satisfaction with service speed and reliability may help retain fiber optic users.
     
● Senior Citizens and Churn: 
   - The analysis reveals that senior citizens (aged 65+) have a churn rate of 41%, compared to a 26% churn rate among non-senior citizens.
   - Implication: Special retention programs and targeted customer service for senior customers may help reduce churn in this demographic.
 ![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/d953507a20e98a4c928504c75db4770777904374/Day%208%20Teco%20Customer%20Churn%20Anlaysis%20Python/Images/Churn%20%25%20by%20SeniorCitizen.jpg)


#### Recommendations:

    - PromoteLong-Term Contracts: Offer incentives for customers to commit to longer contracts to reduce churn. 
    - AddressPayment Method Concerns: Implement campaigns encouraging customers to switch from electronic checks to more reliable payment methods. 
    - CustomerEngagement in Early Tenure: Focus on improving the customer experience within the first year, as churn is highest in this period. 
    - Special Senior Citizen Retention Programs: Create personalized offers or assistance programs to retain the senior demographic.
    
![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/d953507a20e98a4c928504c75db4770777904374/Day%208%20Teco%20Customer%20Churn%20Anlaysis%20Python/Images/Countplot.jpg)

-----------------------------------------------------------------------------------------------------------------------------------------------------------

## Day 9: Blinkit Grocery Store Sales Analysis (Excel)

Using Excel, I designed a dynamic and interactive dashboard providing a deep dive into Blinkit's Grocery sales data. Here's what the dashboard highlights:
![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/811680b98def154f45fb5fa090a85dc41f09a4b2/Day%209%20BlinkIt%20Grocery%20Sales%20Excel/Images/Dashboard.png)

Excel Worksheet: [BlinkIT Grocery Data Excel](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/main/Day%209%20BlinkIt%20Grocery%20Sales%20Excel/BlinkIT%20Grocery%20Data%20Excel.xlsx)

Read Documentation: [Blinkit Grocery Sales Analysis Documentation](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/main/Day%209%20BlinkIt%20Grocery%20Sales%20Excel/Blinkit%20Grocery%20Sales%20Analysis%20Documentation.pdf)

#### Key Insights and Recommendations
 - Focus on High-Selling Categories: Increase marketing efforts for Fruits and Vegetables and Snack Foods, which together contribute nearly 30% of total sales.
 - Expand Regular Fat Content Offerings: As this category constitutes 64.6% of total sales, diversifying products within this segment can further boost revenue.
 - Tier 1 Locations and High-Size Outlets: Invest in these high-performing areas and outlet types to maximize returns.
 - Stabilize Seasonal Trends: Identify reasons behind sales fluctuations and implement strategies like discounts or promotional campaigns to sustain consistent revenue across all years.
 - Customer Retention: Maintain the high average customer rating of 4.0 by focusing on quality and timely delivery, ensuring continued customer satisfaction.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## Day 10: Electric Vehicle Data Analysis(tableau)

The is an in-depth analysis of electric vehicle (EV) adoption across different dimensions, including EV types, vehicle make, model, and state-wise distribution. With 149,771 total EVs(2011 - 2023), the dataset highlights significant trends and insights about the growing EV market.

![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/9a2b4493638c2152feb495a50691063c145922bd/Day%2010%20Electric%20Vehicle%20Data%20Analysis/Images/Dashboard.png)

### Dataset
Download Dataset from kaggle: [Dataset](https://www.kaggle.com/code/vencerlanz09/electric-cars-eda-with-feature-engineering/input)


#### Key Insights and Market Trends
- Battery Electric Vehicles (BEVs) account for 116,474 units, making up 77.8% of the total EVs whereas Plug-in Hybrid Electric Vehicles (PHEVs) contribute 33,297 units or 22.2% of the total EVs.
- EV adoption has grown significantly since 2011. Total vehicle registrations rose from 0.8K in 2011 to 37.1K in 2023, representing a 4,538% increase.
  ![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/9a2b4493638c2152feb495a50691063c145922bd/Day%2010%20Electric%20Vehicle%20Data%20Analysis/Images/Growth%20Rate.png)
- Tesla dominates the market with 68,939 vehicles (57.6%) of the total EVs.
  ![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/9a2b4493638c2152feb495a50691063c145922bd/Day%2010%20Electric%20Vehicle%20Data%20Analysis/Images/Screenshot%202025-01-02%20172243.png)
- 41.87% (62,711 vehicles) are CAFV eligible, and 11.77% (17,634 vehicles) are not CAFV eligible, indicating room for improvement in meeting CAFV standards.

#### Recommendations
 - Diversify Model Offerings:
    - Encourage other manufacturers to innovate and compete with Tesla to diversify market options.
    - Promote more affordable models for increased accessibility.
 - Range Optimization:
    - Invest in R&D to improve EV range and address consumer concerns about distance limitations.
    - Highlight vehicles with exceptional range to attract new customers.


------------------------------------------------------------------------------------------------------------------------------------------------------------

## Day 11: Automobiles Pricing Analysis (Python: Data Wrangling & Model Development)

### Part 1: Data Wrangling
I used data wrangling to covert raw data from initial format to a format that may be better for analysis and future model development.
#### Objectives:
 - Handling missing values
 - Correct data formatting
 - Normalize data

#### Table of Contents
 - Identify missing values
   - Identify missing values
   - Deal with missing values
   - Correct data format
 - Data Normalization (centering/scaling)
 - Binning
 - Indicator Variable

Dataset: [Dowload Dataset here](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/main/Day%2011%20Car%20Pricing%20Analysis%20Python/Dataset_Raw.csv)


### Part 2: Model Development

In this task, I developed several models that predicted the price of the car using the variables or features. This is just an estimate but should give us an objective idea of how much the car should cost. A model will help us understand the exact relationship between different variables and how these variables are used to predict the result.

#### Objectives:
 - Develop prediction models

#### Table of Contents
 - Linear Regression and Multiple Linear Regression
 - Model Evaluation using Visualization
 - Polynomial Regression
 - Pipeline
 - Measure for In-Sample Evaluation
 - Prediction and Decision Making

---------------------------------------------------------------------------------------------------------------
## Day 12: Diwali Sales Report (Excel, Python, Power BI, SQL)

 I delved deep into the fascinating world of festival sales to uncover valuable insights. from analyzing gender and occupation preferences to exploring the impact of marital status and age group on product categories, I thoroughly explored Diwali shopping trends to understand them completely.
![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/e0b2aa652f6aca743fc1ea52e01e4c5bf2834106/Day%2012%20Diwali%20Sales%20Analysis/Images/Dashboard.png)

The first part of this project is Excel: I used Excel to clean my dataset and make it ready for Visualization. Through this visualization, one can simply know what hero products are, for what age group, what kind of occupation, and location of customer.

The second part of my project is using of SQL: I rewind my SQL knowledge to clean the dataset and make it ready for analysis.
Objectives: The objective of this process was to clean and prepare the raw Diwali sales dataset for analysis by addressing missing values, duplicates, and other inconsistencies.

##### Key Recommendations:
 1. Target Marketing Campaigns:
   - Focus on female customers and the 18–35 age group.
   - Offer exclusive promotions for IT and healthcare professionals.
 2. Geographical Expansion:
   - Strengthen presence in top-performing states while improving reach in underperforming regions.
 3. Product Strategy:
   - Stock up on high-demand products like food and clothing during festive seasons.
   - Promote low-performing categories like Tupperware and stationery through bundled offers.
 4. Loyalty Programs:
  - Develop loyalty initiatives for top customers to ensure retention and increased lifetime value.

-------------------------------------------------------------------------------------------------
## Day 15: Heart Failure Prediction

Using a variety of machine learning models such as Random Forest, K-Nearest Neighbor, Decision Tree and others, I analyzed the dataset and identified the Decision Tree models as the best-performing algorithm with an impressive accuracy of 88.9%.
![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/e0b2aa652f6aca743fc1ea52e01e4c5bf2834106/Day%2015%20Hearth%20Failure%20Prediction/Images/Screenshot%202025-01-15%20132350.png)

Key Highlights:
- Data Analysis: Segmented the dataset into categorical and continuous variables for targeted analysis.
- Visualization Insights: Leveraged donut chart to analyze categorical data and highlight trends.
- Model Evaluation: Tested multiple models and compared their performance to select the most accurate predictor.
- Technical Stack: Python, Pandas, NumPy, Matplotlib, Seaborn, Scikit-Learn


-----------------------------------------------------------------------------------------------
## Day 17: Coffee Sales Report(Power BI, Excel, & SQL)
[Documentation](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/main/Day%2017%20Coffee%20Sales%20Analysis/Coffee%20Sales%20Documentation.pdf)

The goal of analyzing transaction data is to get important insights regarding customer behavior,
product popularity, sales patterns, and operational efficiencies. The purpose is to optimize 
inventory management, improve decision-making processes, and find possible cross-selling opportunities.

![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/8a4efa1ef0dc8f81d03fd63107fb946990f063fd/Day%2017%20Coffee%20Sales%20Analysis/Dashboard%20Picture.png)


#### Project Task:
- Data collection, cleaning, and preparation
- Analyse monthly, daily, and hourly sales patterns with Power PivotTables.
- Determine high-performing days and times.
- Develop compelling reports and visualization
- Create an Interactive dashboard

#### Tools used:
- Excel
- Power BI
- MS SQL

### Dataset:
[Download Dataset](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/main/Day%2017%20Coffee%20Sales%20Analysis/Coffee%20Shop%20Sales.xlsx)

#### Key Takeaways:
- Top Sellers: Coffee leads with 25% of total sales, followed by Bakery and Tea.
- Store Performance: Our Astoria and Hell’s Kitchen locations are top performers, each contributing over 34% of total sales.
- Monthly Trends: We find the steady growth in each monthly continuously increasing their sales.
- Customer Habits: Peak sales times are between 8 AM and 11 AM on weekdays, aligning with the morning coffee rush.
- Product Trends: Coffee beans and branded products saw the highest month-on-month growth, while sustainably grown coffee remains our top-seller.
 
#### Conclusion:
  Peak transaction times, high-performing days, and areas for improvement are identified. By leveraging these findings,you can enhance customer experiences, refine inventory management, and boost overall sales efficiency for a thriving coffee shop venture.


-----------------------------------------------------------------------------------------------------------------------------------------------------------

## Day 18: Bank Loan Report (Excel & Power BI)
This dashboard provides a comprehensive analysis of the bank’s loan performance, highlighting key metrics such as total loan applications, loan amounts, amount received, interest rates, and debt-to-income ratio (DTI).
![Power BI Dashboard pdf](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/853ad1524f652f3e808bac170760c9632935f65c/Day%2018%20Bank%20Loan%20Report%20Excel/Bank%20Loan%20Report/Bank%20Loan%20Report%20pdf.pdf)

#### Dashboard Summary
![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/853ad1524f652f3e808bac170760c9632935f65c/Day%2018%20Bank%20Loan%20Report%20Excel/Images/Dashboard%20Summary.png)
![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/853ad1524f652f3e808bac170760c9632935f65c/Day%2018%20Bank%20Loan%20Report%20Excel/Images/Dashboard%20Overview.png)

#### Dataset
[Download Dataset](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/main/Day%2018%20Bank%20Loan%20Report%20Excel/Loan_Dataset.xlsx)

### Key Highlights
 - Total Loan Applications: 38.6K, with 4.3K applications received MTD (Month-to-Date), reflecting 6.9% growth.
 - Total Loan Amount Funded: $435.8M, with $54.0M funded MTD, indicating 13.0% increase.
 - Total Amount Received: $473.1M, with $58.1M received MTD, showing 15.8% increase.
 - Average Interest Rate: 12.05%, slightly lower than MTD rate of 12.36%.
 - Average Debt-to-Income Ratio (DTI): 13.33%, with an MTD reduction to 13.67% (-2.7%).

### Loan Performance Analysis
- 1. Good Loan Issued: Strong Performance
    - 85.9% of total loans are classified as good loans, demonstrating high-quality lending practices.
    - 33.2K good loan applications resulted in a total funded amount of $370.2M, with $435.8M successfully received.
  - Key Strengths:
    - Strong repayment trends.
    - Efficient fund disbursement and collection.
    - High proportion of fully paid loans.
- 2. Bad Loan Issued: Areas of Concern
    - 14.1% of loans are bad loans, indicating some degree of credit risk.
    - 5.3K applications resulted in a funded amount of $65.5M, but only $37.3M has been recovered, leading to potential losses.
  - Key Concerns:
    - Higher non-performing loan (NPL) ratio in certain segments.
    - Potential need for stricter credit risk assessments.

#### Areas of Improvement & Recommendations
 - 1. Reduce Bad Loan Issuance (14.1%)
    - Implement stricter credit checks and borrower risk assessments.
    - Strengthen loan approval processes to minimize high-risk lending.
    - Offer financial literacy programs to borrowers.
 - 2. Improve Loan Recovery & Reduce Charge-Offs
    - Enhance debt collection strategies.
    - Introduce flexible repayment options for struggling borrowers.
    - Use predictive analytics to detect potential defaults early.
 - 3. Optimize Interest Rates & DTI Ratio
    - Adjust interest rates based on borrower credit scores and risk factors.
    - Maintain DTI ratio below 13% to ensure borrower affordability.
 - 4. Enhance Loan Portfolio Diversification
    - Expand lending to low-risk segments with stable income sources.
    - Diversify loan products to cater to different borrower profiles.
 - 5. Increase Efficiency in Loan Disbursement & Collection
    - Automate loan approval and repayment tracking for faster processing.
    - Leverage AI-based fraud detection to prevent high-risk lending.

-------------------------------------------------------------------------------------------------------------------

## Day 20: HR Analytic Dashboard (Tableau & Power BI)

I've developed an interactive Tableau & Power BI dashboard that focuses on employee attrition trends. This dashboard provides a deep dive into attrition data, enabling HR teams to identify key factors contributing to turnover and take proactive steps to improve retention strategies.

[Tableau](https://public.tableau.com/app/profile/aayush.chhettri/viz/HRAnalyticDashboard_17386802426140/HRDashboard) :  ![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/328bceb1b833e819fe7362bd07ef32160b8d8793/Day%2020%20HR%20Dashboard%20Tableau/Images/Tableau%20Dashboard.png)

[Download Dataset](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/main/Day%2020%20HR%20Dashboard%20Tableau/HR%20Data.xlsx)

![alt text](https://github.com/Aayush-Basnet/31-Day-of-Data-Analytic-Project/blob/328bceb1b833e819fe7362bd07ef32160b8d8793/Day%2020%20HR%20Dashboard%20Tableau/Images/HR%20Summary%20Dashboard.png)

This dashboard provides a clear and concise view for HR teams, aiding in decision-making process related to promotions, retrenchments, and workforce distribution.


----------------------------------------------------------------------------------------------------------------------------------

## Day 21: Spotify Data Analysis (SQL)

This project involves analyzing a spotify dataset having various attributes about albums, tracks, streams, views, likes, artists and other components using SQL. It covers the end-to-end process of normalizing a denormalized dataset, performing SQL queries of varying complexity (easy, medium, and advanced), and optimizing query performance. 
The primary goal of the project is to practice advanced SQL skills and generate valuable insights from the dataset.

[Download Dataset](https://www.kaggle.com/datasets/sanjanchaudhari/spotify-dataset)

#### 15 Practice Questions

 ### Easy Level
   - Retrieve the names of all tracks that have more than 1 billion streams.
   - List all albums along with their respective artists.
   - Get the total number of comments for tracks where licensed = TRUE.
   - Find all tracks that belong to the album type single.
   - Count the total number of tracks by each artist.

 ### Medium Level
   - Calculate the average danceability of tracks in each album.
   - Find the top 5 tracks with the highest energy values.
   - List all tracks along with their views and likes where official_video = TRUE.
   - For each album, calculate the total views of all associated tracks.
   - Retrieve the track names that have been streamed on Spotify more than YouTube.

 ### Advanced Level
   - Find the top 3 most-viewed tracks for each artist using window functions.
   - Write a query to find tracks where the liveness score is above the average.
   - Use a WITH clause to calculate the difference between the highest and lowest energy values for tracks in each album.
   - Find tracks where the energy-to-liveness ratio is greater than 1.2.
   - Calculate the cumulative sum of likes for tracks ordered by the number of views, using window functions.

